{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documention for Ezt ez-transform (Ezt) - Analytics engineering for data lakes. Concept ezt aims to provide a powerful and simple analytics engineering experience (similar to dbt ) for building data models on a data lake, a Data Lakehouse if you will, without having to use a third-party processing engine to execute the computations. Ezt is powered by polars , arrow and delta-rs . Ezt provides you with: Standardized project template Possibility to use fully-fledged software development practices, such as version control, CI/CD, unit testing etc. CLI-based interface for easy use in CI/CD pipelines Freedom to choose your own compute engine Deltalake support through delta-rs Take a look at the Guides -section in order to learn how to install Ezt and build your first models.","title":"Home"},{"location":"#welcome-to-the-documention-for-ezt","text":"ez-transform (Ezt) - Analytics engineering for data lakes.","title":"Welcome to the documention for Ezt"},{"location":"#concept","text":"ezt aims to provide a powerful and simple analytics engineering experience (similar to dbt ) for building data models on a data lake, a Data Lakehouse if you will, without having to use a third-party processing engine to execute the computations. Ezt is powered by polars , arrow and delta-rs . Ezt provides you with: Standardized project template Possibility to use fully-fledged software development practices, such as version control, CI/CD, unit testing etc. CLI-based interface for easy use in CI/CD pipelines Freedom to choose your own compute engine Deltalake support through delta-rs Take a look at the Guides -section in order to learn how to install Ezt and build your first models.","title":" Concept"},{"location":"pages/faq/","text":"Frequently asked questions (FAQ) 1. Can I write my models as SQL? At the moment, SQL is not supported in Ezt. However there are plans to support SQL in the future to lower the barrier of entry for engineers not familiar with dataframe manipulation. 2. Why is Polars preferred instead of a more popular dataframe library, such as Pandas or Spark? Polars has been chosen as the main data manipulation library since it supports paralell execution and is very memory efficient, which allows users to work with much larger datasets and achieve a lot faster processing speeds compared to Pandas. Spark is a distributed analytics engine, meaning that it is meant to run on clusters instead of a single machine. The goal of Ezt is not to act as a substitute to Spark, but rather as a compliment for use cases where the distributed capabilities of Spark are not needed. 3. How much data can I process with Ezt? A lot . But it all depends on the machine/server on which you are running your models. One of the goals of Ezt is to allow you to run the majority of your data engineering pipelines on low-cost services such as serverless container services. In the future, we hope to provide some benchmarks so that the user is able to better make decisions on what infrastructure to choose for running Ezt.","title":"FAQ"},{"location":"pages/faq/#frequently-asked-questions-faq","text":"","title":"Frequently asked questions (FAQ)"},{"location":"pages/faq/#1-can-i-write-my-models-as-sql","text":"At the moment, SQL is not supported in Ezt. However there are plans to support SQL in the future to lower the barrier of entry for engineers not familiar with dataframe manipulation.","title":"1. Can I write my models as SQL?"},{"location":"pages/faq/#2-why-is-polars-preferred-instead-of-a-more-popular-dataframe-library-such-as-pandas-or-spark","text":"Polars has been chosen as the main data manipulation library since it supports paralell execution and is very memory efficient, which allows users to work with much larger datasets and achieve a lot faster processing speeds compared to Pandas. Spark is a distributed analytics engine, meaning that it is meant to run on clusters instead of a single machine. The goal of Ezt is not to act as a substitute to Spark, but rather as a compliment for use cases where the distributed capabilities of Spark are not needed.","title":"2. Why is Polars preferred instead of a more popular dataframe library, such as Pandas or Spark?"},{"location":"pages/faq/#3-how-much-data-can-i-process-with-ezt","text":"A lot . But it all depends on the machine/server on which you are running your models. One of the goals of Ezt is to allow you to run the majority of your data engineering pipelines on low-cost services such as serverless container services. In the future, we hope to provide some benchmarks so that the user is able to better make decisions on what infrastructure to choose for running Ezt.","title":"3. How much data can I process with Ezt?"},{"location":"pages/guides/analyzing_data/","text":"Analyzing data A drawback of a data lake in comparison to a data warehouse, is the lack of a query interface. There is no built-in functionality through which you could easily query and analyze a particular dataset located in a data lake. Teams usually solve this either by using spark or some kind of query engine, like AWS Athena. Ezt tries to solve this problem by letting users utilize the get_source and get_model to analyze data in for example notebooks without having to write all the necessary code to read a specific file type from a specific filesystem. The get_source and get_model can be called from any working directory by specifying the optional parameter project_base_dir , which should be a path to the directory where your ezt_project.yml is located. For example, you could have a .notebooks -folder inside your ezt-project with notebooks that contains data analysis on your sources and models. This is an example how to use the get_source function to read a source you have defined in your sources.yml located in some remote storage supported by Ezt (currently S3 and Azure ADLS Gen2). from ezt import get_source , get_model from dotenv import load_dotenv # load environment variables from .env -file for authentication to remote storage load_dotenv () # fetch source by name defined in source.yml my_lazyframe = get_source ( name = \"my_source\" , project_base_dir = \"../\" ) After that you can query your data however your want.","title":"Analyzing data"},{"location":"pages/guides/analyzing_data/#analyzing-data","text":"A drawback of a data lake in comparison to a data warehouse, is the lack of a query interface. There is no built-in functionality through which you could easily query and analyze a particular dataset located in a data lake. Teams usually solve this either by using spark or some kind of query engine, like AWS Athena. Ezt tries to solve this problem by letting users utilize the get_source and get_model to analyze data in for example notebooks without having to write all the necessary code to read a specific file type from a specific filesystem. The get_source and get_model can be called from any working directory by specifying the optional parameter project_base_dir , which should be a path to the directory where your ezt_project.yml is located. For example, you could have a .notebooks -folder inside your ezt-project with notebooks that contains data analysis on your sources and models. This is an example how to use the get_source function to read a source you have defined in your sources.yml located in some remote storage supported by Ezt (currently S3 and Azure ADLS Gen2). from ezt import get_source , get_model from dotenv import load_dotenv # load environment variables from .env -file for authentication to remote storage load_dotenv () # fetch source by name defined in source.yml my_lazyframe = get_source ( name = \"my_source\" , project_base_dir = \"../\" ) After that you can query your data however your want.","title":"Analyzing data"},{"location":"pages/guides/installation/","text":"Installation Requirements In order to use Ezt the following is required: Python 3.7 or newer Linux You can install Ezt with pip simply by writing: pip install ez-transform We recommend installing Ezt in a python virtual environment. MacOS To be added. Windows To be added.","title":"Installation"},{"location":"pages/guides/installation/#installation","text":"","title":"Installation"},{"location":"pages/guides/installation/#requirements","text":"In order to use Ezt the following is required: Python 3.7 or newer","title":"Requirements"},{"location":"pages/guides/installation/#linux","text":"You can install Ezt with pip simply by writing: pip install ez-transform We recommend installing Ezt in a python virtual environment.","title":"Linux"},{"location":"pages/guides/installation/#macos","text":"To be added.","title":"MacOS"},{"location":"pages/guides/installation/#windows","text":"To be added.","title":"Windows"},{"location":"pages/guides/model_creation/","text":"Creating a model The main way to create a model in Ezt, is to have existing datasets somewhere that are defined in your Ezt project as sources and build a data model on top of those where the datasets get joined, aggrated, filtered, pivoted etc. Ezt also allows chaining of models. This means that existing models can function as input for other models. When using the built-in get_source and get_model functions to fetch datasets in your model, Ezt automatically calculates the correct order in which to execute the models when calling ezt run from the command line. If one model depends on another model, Ezt first processes and persists the first model in the chain, and then proceeds to process and persist the second model. Models without dependencies on other models gets processed in parallell. To achieve this, Ezt uses the concept of a directed acyclical graph (DAG), which get calculated at runtime, in order to determine which models can be processed in parallell and which models need to be processed before another model. Model definition In order to create a model, the user needs to define the model in a models.yml file (similar to how sources are defined) and then create a .py -file containing a function named df_model that returns a Polars DataFrame. It is entirely up to the user to define the logic that creates that DataFrame, either by putting all logic inside the df_model function, or using other functions or classes defined somewhere else in the project, to handle the calculations producing the DataFrame that then gets returned by the df_model function. Let's first look at the yml-configuration. Defining the model configuration The model configuration tells Ezt the type of the model and how the model should be persisted. A models.yml can look something like this: models.yml models : - name : product_metrics #(1) filesystem : s3 #(2) type : df #(3) destination : my-ezt-bucket/product #(4) write_settings : #(5) file_type : parquet #(6) mode : overwrite #(7) Just like a source, a model requires a unique name. Required key Filesystems supported right now are local filesystem and s3 . Required key Ezt currently only supports DataFrame models defined in python. There are plans to also support SQL models in the future. Required key Destination tells Ezt where to store the model. This path should always be pointing to a folder. Required Write settings is a yml dictionary that handles how the model should be persisted. Required key Currently, parquet and delta are supported as format for storing models. Required key The mode can be either overwrite , append or merge . Required key Modes Ezt supports the modes append , overwrite or merge . Overwrite simply overwrites the existing model with the new result. Append adds all rows of the new result to the existing result. When mode is set to merge , an additional key named merge_col is needed to be able to calculate which rows should be updated and which rows should be inserted. At the moment, deletes are not supported and needs to be handled entirely by the user. For example, having a column in your dataset set to either True or False depending on if the given row is deleted or not is a good strategy. Defining the python model When you have created the model definition and decided where and how to persist your data, you can start building your model. The model should be created in a python file named model_name.py (where model_name is the name of your model defined in models.yml ) and return a Polars DataFrame. Polars is installed as a dependency to Ezt so no need to install it explicitly. The function returning the Polars DatFrame should be decorated with @py_model and be named df_model . A model can look something like this: product_metrics.py from ezt import py_model , get_source @py_model def df_model (): products_lf = get_source ( 'products_raw' ) sales_lf = get_source ( 'sales_raw' ) query = ( products_lf . join ( sales_lf , on = 'ProductKey' , how = 'inner' ) . select ( [ pl . col ( \"ProductKey\" ), pl . col ( \"ProductName\" ), pl . col ( \"OrderQuantity\" ), pl . col ( \"ProductCost\" ), pl . col ( \"ProductPrice\" ), ( pl . col ( \"ProductPrice\" ) - pl . col ( \"ProductCost\" )) . alias ( \"UnitMargin\" ), ( pl . col ( \"ProductPrice\" ) * pl . col ( \"OrderQuantity\" )) . alias ( 'TotalSalesAmount' ), ] ) ) return query . collect () In this model, two sources named \"products_raw\" and \"sales_raw\" are first fetched by using the get_source -function. Beware that the get_source and get_model functions return a Polars LazyFrame and not a DataFrame. The query then performs a join and then a select where two additional columns get calculated and added to the result. Since the computations get done on LazyFrames, we need to call collect() on the result before returning it to return a DataFrame. If Polars syntax is unfamiliar, you can check out the Polars API docs and the Polars User Guide to learn more. In practice, one can use any library they want to perform the computations on the data, as long as the df_model function returns a Polars DataFrame. Polars is currently one of the fastest and most memory efficient DataFrame libraries existing for single machines and is therefore recommended. Source: h2oai's db-benchmark . Next, check out the guide for how to structure your project.","title":"Creating a model"},{"location":"pages/guides/model_creation/#creating-a-model","text":"The main way to create a model in Ezt, is to have existing datasets somewhere that are defined in your Ezt project as sources and build a data model on top of those where the datasets get joined, aggrated, filtered, pivoted etc. Ezt also allows chaining of models. This means that existing models can function as input for other models. When using the built-in get_source and get_model functions to fetch datasets in your model, Ezt automatically calculates the correct order in which to execute the models when calling ezt run from the command line. If one model depends on another model, Ezt first processes and persists the first model in the chain, and then proceeds to process and persist the second model. Models without dependencies on other models gets processed in parallell. To achieve this, Ezt uses the concept of a directed acyclical graph (DAG), which get calculated at runtime, in order to determine which models can be processed in parallell and which models need to be processed before another model.","title":"Creating a model"},{"location":"pages/guides/model_creation/#model-definition","text":"In order to create a model, the user needs to define the model in a models.yml file (similar to how sources are defined) and then create a .py -file containing a function named df_model that returns a Polars DataFrame. It is entirely up to the user to define the logic that creates that DataFrame, either by putting all logic inside the df_model function, or using other functions or classes defined somewhere else in the project, to handle the calculations producing the DataFrame that then gets returned by the df_model function. Let's first look at the yml-configuration.","title":"Model definition"},{"location":"pages/guides/model_creation/#defining-the-model-configuration","text":"The model configuration tells Ezt the type of the model and how the model should be persisted. A models.yml can look something like this: models.yml models : - name : product_metrics #(1) filesystem : s3 #(2) type : df #(3) destination : my-ezt-bucket/product #(4) write_settings : #(5) file_type : parquet #(6) mode : overwrite #(7) Just like a source, a model requires a unique name. Required key Filesystems supported right now are local filesystem and s3 . Required key Ezt currently only supports DataFrame models defined in python. There are plans to also support SQL models in the future. Required key Destination tells Ezt where to store the model. This path should always be pointing to a folder. Required Write settings is a yml dictionary that handles how the model should be persisted. Required key Currently, parquet and delta are supported as format for storing models. Required key The mode can be either overwrite , append or merge . Required key","title":"Defining the model configuration"},{"location":"pages/guides/model_creation/#modes","text":"Ezt supports the modes append , overwrite or merge . Overwrite simply overwrites the existing model with the new result. Append adds all rows of the new result to the existing result. When mode is set to merge , an additional key named merge_col is needed to be able to calculate which rows should be updated and which rows should be inserted. At the moment, deletes are not supported and needs to be handled entirely by the user. For example, having a column in your dataset set to either True or False depending on if the given row is deleted or not is a good strategy.","title":"Modes"},{"location":"pages/guides/model_creation/#defining-the-python-model","text":"When you have created the model definition and decided where and how to persist your data, you can start building your model. The model should be created in a python file named model_name.py (where model_name is the name of your model defined in models.yml ) and return a Polars DataFrame. Polars is installed as a dependency to Ezt so no need to install it explicitly. The function returning the Polars DatFrame should be decorated with @py_model and be named df_model . A model can look something like this: product_metrics.py from ezt import py_model , get_source @py_model def df_model (): products_lf = get_source ( 'products_raw' ) sales_lf = get_source ( 'sales_raw' ) query = ( products_lf . join ( sales_lf , on = 'ProductKey' , how = 'inner' ) . select ( [ pl . col ( \"ProductKey\" ), pl . col ( \"ProductName\" ), pl . col ( \"OrderQuantity\" ), pl . col ( \"ProductCost\" ), pl . col ( \"ProductPrice\" ), ( pl . col ( \"ProductPrice\" ) - pl . col ( \"ProductCost\" )) . alias ( \"UnitMargin\" ), ( pl . col ( \"ProductPrice\" ) * pl . col ( \"OrderQuantity\" )) . alias ( 'TotalSalesAmount' ), ] ) ) return query . collect () In this model, two sources named \"products_raw\" and \"sales_raw\" are first fetched by using the get_source -function. Beware that the get_source and get_model functions return a Polars LazyFrame and not a DataFrame. The query then performs a join and then a select where two additional columns get calculated and added to the result. Since the computations get done on LazyFrames, we need to call collect() on the result before returning it to return a DataFrame. If Polars syntax is unfamiliar, you can check out the Polars API docs and the Polars User Guide to learn more. In practice, one can use any library they want to perform the computations on the data, as long as the df_model function returns a Polars DataFrame. Polars is currently one of the fastest and most memory efficient DataFrame libraries existing for single machines and is therefore recommended. Source: h2oai's db-benchmark . Next, check out the guide for how to structure your project.","title":"Defining the python model"},{"location":"pages/guides/project_initiation/","text":"Project initiation When using Ezt you will be editing files locally using a code editor, and running projects using the terminal on your local machine. Prerequisites In order to use Ezt, we recommend that the user understands the following concepts: Basic understanding of how to use the Terminal. Basic Python programming language understanding. Ezt encourages users to use Polars as the main data manipulation module for your data models, so basic understanding of Polars is needed to build a data model. Polars syntax is not very different compared to Pandas or Spark, but some differences exist. Fortunately Polars has excellent API-documentation at https://pola-rs.github.io/polars/py-polars/html/reference/ as well as a great book documentation at https://pola-rs.github.io/polars-book/user-guide/index.html . The recommended way to create a new Ezt project is the following: 1. From your terminal, create a python virtual environment. 2. Activate your virtual environment and install Ezt with the following command: pip install ez-transform 3. Check that Ezt is accessible from your terminal simply by running the command 'ezt'. You should see a \"Welcome to Ezt!\" -message and some command suggestions. If you see a message indicating that the 'ezt'-command cannot be found, you will need to add it to your Path environment variable. 4. Change directory to the place where you want your local directory stored. E.g.: cd /home/NinjaTurtle/EztProjects 5. Initiate a new Ezt project by writing 'ezt init project-name ' and replace project-name with the name of your choice, like this: ezt init myproject 6. The previous command will create a new folder myproject with a baseline Ezt project structure inside the myproject folder. Now, cd into your project folder just created. 7. When inside your project folder, run the following command to ensure that everything is in order inside your project: ezt check This command will validate all of the configuration files you have inside your project and return an error message if something is not right. Since you haven't done any changes to your project yet, this command should pass with some thumbs up emojis and messages indicating that some yml-files have been successfully validated.","title":"Project initiation"},{"location":"pages/guides/project_initiation/#project-initiation","text":"When using Ezt you will be editing files locally using a code editor, and running projects using the terminal on your local machine.","title":"Project initiation"},{"location":"pages/guides/project_initiation/#prerequisites","text":"In order to use Ezt, we recommend that the user understands the following concepts: Basic understanding of how to use the Terminal. Basic Python programming language understanding. Ezt encourages users to use Polars as the main data manipulation module for your data models, so basic understanding of Polars is needed to build a data model. Polars syntax is not very different compared to Pandas or Spark, but some differences exist. Fortunately Polars has excellent API-documentation at https://pola-rs.github.io/polars/py-polars/html/reference/ as well as a great book documentation at https://pola-rs.github.io/polars-book/user-guide/index.html . The recommended way to create a new Ezt project is the following: 1. From your terminal, create a python virtual environment. 2. Activate your virtual environment and install Ezt with the following command: pip install ez-transform 3. Check that Ezt is accessible from your terminal simply by running the command 'ezt'. You should see a \"Welcome to Ezt!\" -message and some command suggestions. If you see a message indicating that the 'ezt'-command cannot be found, you will need to add it to your Path environment variable. 4. Change directory to the place where you want your local directory stored. E.g.: cd /home/NinjaTurtle/EztProjects 5. Initiate a new Ezt project by writing 'ezt init project-name ' and replace project-name with the name of your choice, like this: ezt init myproject 6. The previous command will create a new folder myproject with a baseline Ezt project structure inside the myproject folder. Now, cd into your project folder just created. 7. When inside your project folder, run the following command to ensure that everything is in order inside your project: ezt check This command will validate all of the configuration files you have inside your project and return an error message if something is not right. Since you haven't done any changes to your project yet, this command should pass with some thumbs up emojis and messages indicating that some yml-files have been successfully validated.","title":"Prerequisites"},{"location":"pages/guides/project_structure/","text":"Project structure When initiating an Ezt-project, it will automatically create some files and folder for you. The structure of simple Ezt project will look something like this: /myproject | \u2014\u2014\u2014 __init__.py | \u2014\u2014\u2014 .gitignore | \u2014\u2014\u2014 README.md | \u2014\u2014\u2014 sources.yml #(1) | \u2014\u2014\u2014 ezt_project.yml #(2) | \u2014\u2014\u2014 models #(3) | | \u2014\u2014\u2014 __init__.py | | \u2014\u2014\u2014 models.yml #(4) | | \u2014\u2014\u2014 my_model.py #(5) | | \u2014\u2014\u2014 staging_models #(6) | | | \u2014\u2014\u2014 __init__.py | | | \u2014\u2014\u2014 models_staging.yml | | | \u2014\u2014\u2014 stage_1.py | | | \u2014\u2014\u2014 stage_2.py This is where you put your sources. This file contains various project-specific configurations. Such as the folder structure for models inside the project and if logging should be enabled or not. This is the default folder in which you create your model yml-configurations and your python files. This is where your model configurations go. This is where your model code go. You can also put your models into sub-folders to be able to structure what code goes where better. Sources Sources always go inside the sources.yml file and should be located in the base directory of your project. There are plans to add functionality that would allow users to structure sources better in the future. ezt_project.yml The ezt_project.yml currently determines the following things: Name of project. Structure of models. Logging destination. Example ezt_project.yml ezt_project.yml name : 'my_project' #(1) models : #(2) main_folder : models #(3) groups : #(4) - name : sales - name : products logs_destination : /home/john/ezt-test/logs #(5) Name of project The models-key tells Ezt how models are structured. main_folder points to the base folder of your model-structure. groups tells Ezt how many folders containing models that are inside the main_folder. The names specified here has to match with the folder names. Path to where execution logs should be stored. Models As seen from the ezt_project.yml example above, models can be structured into a hierarchy of folders with 2 levels, the main_folder and unlimited number of groups inside that. Each model-folder has to contain: __init__.py models*.yml some text after One python-file per model defined in models.yml. models.yml in model groups The file containing model configuration can in a model group be named models*.yml , where the star can be any word or character. This allow the user to distinguish between model.yml files if they are working with many. For example in a group named sales , the yml-file can be named for example models_sales.yml .","title":"Structuring your project"},{"location":"pages/guides/project_structure/#project-structure","text":"When initiating an Ezt-project, it will automatically create some files and folder for you. The structure of simple Ezt project will look something like this: /myproject | \u2014\u2014\u2014 __init__.py | \u2014\u2014\u2014 .gitignore | \u2014\u2014\u2014 README.md | \u2014\u2014\u2014 sources.yml #(1) | \u2014\u2014\u2014 ezt_project.yml #(2) | \u2014\u2014\u2014 models #(3) | | \u2014\u2014\u2014 __init__.py | | \u2014\u2014\u2014 models.yml #(4) | | \u2014\u2014\u2014 my_model.py #(5) | | \u2014\u2014\u2014 staging_models #(6) | | | \u2014\u2014\u2014 __init__.py | | | \u2014\u2014\u2014 models_staging.yml | | | \u2014\u2014\u2014 stage_1.py | | | \u2014\u2014\u2014 stage_2.py This is where you put your sources. This file contains various project-specific configurations. Such as the folder structure for models inside the project and if logging should be enabled or not. This is the default folder in which you create your model yml-configurations and your python files. This is where your model configurations go. This is where your model code go. You can also put your models into sub-folders to be able to structure what code goes where better.","title":"Project structure"},{"location":"pages/guides/project_structure/#sources","text":"Sources always go inside the sources.yml file and should be located in the base directory of your project. There are plans to add functionality that would allow users to structure sources better in the future.","title":"Sources"},{"location":"pages/guides/project_structure/#ezt_projectyml","text":"The ezt_project.yml currently determines the following things: Name of project. Structure of models. Logging destination.","title":"ezt_project.yml"},{"location":"pages/guides/project_structure/#example-ezt_projectyml","text":"ezt_project.yml name : 'my_project' #(1) models : #(2) main_folder : models #(3) groups : #(4) - name : sales - name : products logs_destination : /home/john/ezt-test/logs #(5) Name of project The models-key tells Ezt how models are structured. main_folder points to the base folder of your model-structure. groups tells Ezt how many folders containing models that are inside the main_folder. The names specified here has to match with the folder names. Path to where execution logs should be stored.","title":"Example ezt_project.yml"},{"location":"pages/guides/project_structure/#models","text":"As seen from the ezt_project.yml example above, models can be structured into a hierarchy of folders with 2 levels, the main_folder and unlimited number of groups inside that. Each model-folder has to contain: __init__.py models*.yml some text after One python-file per model defined in models.yml. models.yml in model groups The file containing model configuration can in a model group be named models*.yml , where the star can be any word or character. This allow the user to distinguish between model.yml files if they are working with many. For example in a group named sales , the yml-file can be named for example models_sales.yml .","title":"Models"},{"location":"pages/guides/register_source/","text":"Registering sources The main way to use Ezt is to already have some data located somewhere, on top of which you want to build your data models. In order for Ezt to understand how to fetch and read that data, you need to list information about that source inside the sources.yml file, which . This guide will show you how to register a source. To register a source, you need to have knowledge about the following: Location, such as filesystem and file/folder path. File type Authentication (for remote object stores, like S3) The way to set up a source slightly differs depending on the filesystem and file type. Ezt supports csv, parquet and delta as filetype for sources. The yml definition of a source in S3 can look something like this: sources.yml sources : - name : new_cars #(1) filesystem : s3 #(2) path_type : folder #(3) format : parquet #(4) path : my-bucket/data/new_cars #(5) Every source requires a unique name. Filesystems supported right now are local filesystem and S3 . path_type can be either file or folder . When set to file , the path needs to point to a specific file, and when set to folder , the path needs to point to a folder and every file in that folder will get included in the dataset. File-format in which the data is stored. path points to a specific file or folder depending on the path_type value. So in this example, we have defined a source named new_cars residing in S3 and this is all you need to do in order to register a source. To later use this source inside a data model, you need to have $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY set as environment variables from the shell that runs the ezt run command. Next, check out the guide for creating a model to find out how to reference this source dataset inside a data model.","title":"Registering a source"},{"location":"pages/guides/register_source/#registering-sources","text":"The main way to use Ezt is to already have some data located somewhere, on top of which you want to build your data models. In order for Ezt to understand how to fetch and read that data, you need to list information about that source inside the sources.yml file, which . This guide will show you how to register a source. To register a source, you need to have knowledge about the following: Location, such as filesystem and file/folder path. File type Authentication (for remote object stores, like S3) The way to set up a source slightly differs depending on the filesystem and file type. Ezt supports csv, parquet and delta as filetype for sources. The yml definition of a source in S3 can look something like this: sources.yml sources : - name : new_cars #(1) filesystem : s3 #(2) path_type : folder #(3) format : parquet #(4) path : my-bucket/data/new_cars #(5) Every source requires a unique name. Filesystems supported right now are local filesystem and S3 . path_type can be either file or folder . When set to file , the path needs to point to a specific file, and when set to folder , the path needs to point to a folder and every file in that folder will get included in the dataset. File-format in which the data is stored. path points to a specific file or folder depending on the path_type value. So in this example, we have defined a source named new_cars residing in S3 and this is all you need to do in order to register a source. To later use this source inside a data model, you need to have $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY set as environment variables from the shell that runs the ezt run command. Next, check out the guide for creating a model to find out how to reference this source dataset inside a data model.","title":"Registering sources"},{"location":"pages/guides/run_models/","text":"Running models Running your models is simple, all you need to do is execute the ezt run command from the command line when inside your project directory where your ezt_project.yml is located. This will execute all of your models that you have created inside your project. Check out the CLI reference in order to learn about the options you can use with the ezt run command.","title":"Running models"},{"location":"pages/guides/run_models/#running-models","text":"Running your models is simple, all you need to do is execute the ezt run command from the command line when inside your project directory where your ezt_project.yml is located. This will execute all of your models that you have created inside your project. Check out the CLI reference in order to learn about the options you can use with the ezt run command.","title":"Running models"},{"location":"pages/reference/cli_ref/","text":"CLI reference This page is automatically generate, so it may include some weird formatting. ezt Welcome to ezt! Usage: ezt [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. ezt check Checks if a ezt project is initiated and working. Usage: ezt check [OPTIONS] Options: --help Show this message and exit. ezt init Create a new project in the current folder. Usage: ezt init [OPTIONS] PROJECT_NAME Options: --help Show this message and exit. ezt order Print out the execution order of your ezt models. Usage: ezt order [OPTIONS] Options: --help Show this message and exit. ezt run Runs the models you have created. Usage: ezt run [OPTIONS] Options: --validate Validate yaml files before running models. --model-name TEXT Provide the name of a specific model to run. --model-group TEXT Provide the name of a specific model group to run all models in that group and skip the rest. --help Show this message and exit.","title":"CLI"},{"location":"pages/reference/cli_ref/#cli-reference","text":"This page is automatically generate, so it may include some weird formatting.","title":" CLI reference"},{"location":"pages/reference/cli_ref/#ezt","text":"Welcome to ezt! Usage: ezt [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"ezt"},{"location":"pages/reference/cli_ref/#ezt-check","text":"Checks if a ezt project is initiated and working. Usage: ezt check [OPTIONS] Options: --help Show this message and exit.","title":"check"},{"location":"pages/reference/cli_ref/#ezt-init","text":"Create a new project in the current folder. Usage: ezt init [OPTIONS] PROJECT_NAME Options: --help Show this message and exit.","title":"init"},{"location":"pages/reference/cli_ref/#ezt-order","text":"Print out the execution order of your ezt models. Usage: ezt order [OPTIONS] Options: --help Show this message and exit.","title":"order"},{"location":"pages/reference/cli_ref/#ezt-run","text":"Runs the models you have created. Usage: ezt run [OPTIONS] Options: --validate Validate yaml files before running models. --model-name TEXT Provide the name of a specific model to run. --model-group TEXT Provide the name of a specific model group to run all models in that group and skip the rest. --help Show this message and exit.","title":"run"},{"location":"pages/reference/models/","text":"Model reference The models.yml is the file where you store information about how your models should be written to storage. File format options So far, only yml-files are supported as configuration for model configuration files. Ezt also only supports yml-files with the file extension .yml . Layout The source file has one base key models and under that all of the sources are then gathered as a list. A typical layout with two models would look something like this: models.yml models : #(1) - name : products_model #(2) type : df #(3) filesystem : s3 #(4) destination : ezt-target-dev #(5) write_settings : #(6) file_type : parquet #(7) mode : overwrite #(8) - name : total_car_sales type : df filesystem : local destination : /home/john/ezt-test/destination_data write_settings : file_type : delta mode : merge key : order_id This key is required and the value of it is always a list. This key is required and the value can be basically anything. The name specified here is used when calling the get_model() -function from another model to chain models. This key is required and the value should be df . Sql models will be supported in the future as well. This key is required and is used to specify the filesystem on which the specific model should be created. This key is required and is used to tell Ezt the path to where the models should be stored. This key is required and the value is an object with the keys file_type and mode . This key is required and tells ezt how to persist the model. Values \u00b4parquet\u00b4 and \u00b4delta\u00b4 are supported at the moment, and more will potentially be added in the future. This key is required and tells ezt if processed data should be appended, overwritten or merged. merge requires an additional key under write_settings named merge_key to specify on which column the merge-logic should be calculated. Supported filesystems Ezt currently supports models in the local filesystem and in AWS S3 . If there is a concrete need from the user base, other filesystems such as Azure ADLS Gen 2 and Google Cloud Storage might be supported in the future. Supported file types Ezt currently supports Parquet and delta as file types for models.","title":"Models"},{"location":"pages/reference/models/#model-reference","text":"The models.yml is the file where you store information about how your models should be written to storage.","title":" Model reference"},{"location":"pages/reference/models/#file-format-options","text":"So far, only yml-files are supported as configuration for model configuration files. Ezt also only supports yml-files with the file extension .yml .","title":" File format options"},{"location":"pages/reference/models/#layout","text":"The source file has one base key models and under that all of the sources are then gathered as a list. A typical layout with two models would look something like this: models.yml models : #(1) - name : products_model #(2) type : df #(3) filesystem : s3 #(4) destination : ezt-target-dev #(5) write_settings : #(6) file_type : parquet #(7) mode : overwrite #(8) - name : total_car_sales type : df filesystem : local destination : /home/john/ezt-test/destination_data write_settings : file_type : delta mode : merge key : order_id This key is required and the value of it is always a list. This key is required and the value can be basically anything. The name specified here is used when calling the get_model() -function from another model to chain models. This key is required and the value should be df . Sql models will be supported in the future as well. This key is required and is used to specify the filesystem on which the specific model should be created. This key is required and is used to tell Ezt the path to where the models should be stored. This key is required and the value is an object with the keys file_type and mode . This key is required and tells ezt how to persist the model. Values \u00b4parquet\u00b4 and \u00b4delta\u00b4 are supported at the moment, and more will potentially be added in the future. This key is required and tells ezt if processed data should be appended, overwritten or merged. merge requires an additional key under write_settings named merge_key to specify on which column the merge-logic should be calculated.","title":" Layout"},{"location":"pages/reference/models/#supported-filesystems","text":"Ezt currently supports models in the local filesystem and in AWS S3 . If there is a concrete need from the user base, other filesystems such as Azure ADLS Gen 2 and Google Cloud Storage might be supported in the future.","title":" Supported filesystems"},{"location":"pages/reference/models/#supported-file-types","text":"Ezt currently supports Parquet and delta as file types for models.","title":" Supported file types"},{"location":"pages/reference/sources/","text":"Source file reference The sources.yml is the file where you store information about your source tables that you want to build data models on top of. File format options So far, only yml-files are supported as configuration for source files and other configuration files in Ezt. Ezt also only supports yml-files with the file extension .yml . Layout The source file has one base key sources and under that all of the sources are then gathered as a list. A typical layout with two sources would look something like this: sources.yml sources : #(1) - name : usa_cars #(2) filesystem : local #(3) path_type : file #(4) format : parquet #(5) path : /home/SomeUser/data/usa_cars.parquet - name : car_sales filesystem : s3 path_type : folder format : parquet path : my-bucket/car_sales This key is required and the value of it is always a list. This key is required and the value can be basically anything. The name specified here is used in models when calling the get_source() -function. This key is required and is used to specify the filesystem on which the specific source is stored. This key is required and is used to tell Ezt if the path (specified in the value of the path: key) is a fodler or a file. This key is required and is used to tell Ezt what kind of format the data is stored as. Supported filesystems Ezt currently supports sources in the local filesystem and in AWS S3 . If there is a concrete need from the user base, other filesystems such as Azure ADLS Gen 2 and Google Cloud Storage might be supported in the future. A source in Ezt needs different configuration settings depending on the filesystem where it is stored. Supported file types Ezt currently supports CSV, Parquet and Delta as file types for sources.","title":"Sources"},{"location":"pages/reference/sources/#source-file-reference","text":"The sources.yml is the file where you store information about your source tables that you want to build data models on top of.","title":" Source file reference"},{"location":"pages/reference/sources/#file-format-options","text":"So far, only yml-files are supported as configuration for source files and other configuration files in Ezt. Ezt also only supports yml-files with the file extension .yml .","title":" File format options"},{"location":"pages/reference/sources/#layout","text":"The source file has one base key sources and under that all of the sources are then gathered as a list. A typical layout with two sources would look something like this: sources.yml sources : #(1) - name : usa_cars #(2) filesystem : local #(3) path_type : file #(4) format : parquet #(5) path : /home/SomeUser/data/usa_cars.parquet - name : car_sales filesystem : s3 path_type : folder format : parquet path : my-bucket/car_sales This key is required and the value of it is always a list. This key is required and the value can be basically anything. The name specified here is used in models when calling the get_source() -function. This key is required and is used to specify the filesystem on which the specific source is stored. This key is required and is used to tell Ezt if the path (specified in the value of the path: key) is a fodler or a file. This key is required and is used to tell Ezt what kind of format the data is stored as.","title":" Layout"},{"location":"pages/reference/sources/#supported-filesystems","text":"Ezt currently supports sources in the local filesystem and in AWS S3 . If there is a concrete need from the user base, other filesystems such as Azure ADLS Gen 2 and Google Cloud Storage might be supported in the future. A source in Ezt needs different configuration settings depending on the filesystem where it is stored.","title":" Supported filesystems"},{"location":"pages/reference/sources/#supported-file-types","text":"Ezt currently supports CSV, Parquet and Delta as file types for sources.","title":" Supported file types"}]}